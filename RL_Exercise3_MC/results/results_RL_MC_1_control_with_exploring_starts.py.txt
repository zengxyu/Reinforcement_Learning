We need to estimate Q[s,a].
because Without the environmental model, Q[s,a] can be got by one step ahead knowing V[s].

How to estimate Q[s,a]
Q[s,a] = average(Returns(s,a))

Problems:
some of s,a pair wouldn't appear, because policy pi[s] = argmaxQ[s,a],
policy pi instruct the generation of a in state s. That is, a = argmaxQ[s,a].
so some of s,a pair wouldn't appear.

Solutions:
Exploring starts.

Run 1st round:
Time step i:  900000 ; Converging or not:  True ;delta:  0.00976813026494483
The average length of episodes: 7.8531088888888885
The average total reward of episode: 262.2863477777778
The number of correct policy: 21 /24
State values:
  47.017   57.100   54.306    0.000    0.000    0.000  975.307
  47.101   51.930   71.321   43.244    0.000  755.186    0.000
  41.406    0.000    0.000  157.757    0.000  368.828  453.256
  35.595   31.036    0.000  236.663  251.154    0.000  396.770
  30.930   28.849    0.000  244.602  278.336  320.033  346.711
State values ==================================END

State Counts:
72540.000 166803.000 160389.000 18990.000 75238.000 267874.000 98185.000
152013.000 166999.000 243901.000 101219.000 40141.000 324794.000 131499.000
106844.000 28185.000 104637.000 249560.000 86743.000 107581.000 333974.000
97206.000 22028.000 37724.000 97887.000 170673.000 33422.000 395767.000
57359.000 23608.000 28019.000 91098.000 301463.000 272379.000 347214.000
State Counts ==================================END

Output optimal policy with arrows:
   ↘        ↘        ↓        ●        ●        ●        ↖
   ↗        ↗        ↘        ↓        ●        ↑        ●
   ↑        ●        ●        ↘        ●        ↑        ↖
   ↖        ←        ●        ↘        ↓        ●        ↗
   ↖        ↖        ●        ↘        ↘        →        ↗
Optimal policy ==================================END


The state value is close to that of the optimal policy of value iteration. But it is still not close enough.

